{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Linear Models\n",
    "- [Problem Statement](#Problem-Statement)\n",
    "- [Solution](#Solution)\n",
    "    1. [Initialise Libraries](#initialise-libraries)\n",
    "    2. [Create Helper Functions](#helper-functions)\n",
    "        - [Visualization](#visualization)\n",
    "        - [Data Extraction](#data-extraction)\n",
    "        - [Normalise](#normalise)\n",
    "        - [Model](#model)\n",
    "        - [Loss Function](#loss)\n",
    "        - [Gradient Descent](#gradient-descent)\n",
    "    3. [Train Model](#train-model)\n",
    "        - [Initialise Training Variables](#initialise)\n",
    "        - [Training Script](#training-script)\n",
    "        - [Plot Graph](#plot-graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A real estate consultancy is carrying out an investigation on the trend of recent housing prices based on the size of the house (in square feet) and number of rooms of the property. The data they collected from 47 houses indicates that a linear model that fits this trend. Verify this by plotting the loss of a **multivariate linear model**.\n",
    "\n",
    "$$\\hat{y^{(i)}}=w_{0}+w_{1}*x_{1}^{(i)}+w_{2}*x_{2}^{(i)}$$ \n",
    "\n",
    "In matrix form,\n",
    "\n",
    "$$\\mathbf{\\hat{y}}=\\mathbf{X}\\mathbf{W}=\\begin{bmatrix}1 & x_{1}^{(1)} & x_{2}^{(1)}\\\\ \\vdots& \\vdots&\\vdots\\\\ 1 & x_{1}^{(m)} & x_{2}^{(m)}\\end{bmatrix}\\begin{bmatrix}w_{0} \\\\ w_{1}\\\\w_{2} \\end{bmatrix}$$ \n",
    "\n",
    "The dataset is visualized below: \n",
    "\n",
    "| Size of House (in square feet)  | Number of rooms | Price of house (in USD) |\n",
    "| ------------------------------- | --------------- | ----------------------- |\n",
    "|             2104                |        3        |        399900           |\n",
    "|             1600                |        3        |        329900           |\n",
    "|             2400                |        3        |        369000           |\n",
    "|             1416                |        2        |        232000           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialise Libraries <a class=\"anchor\" id=\"initialise-libraries\"></a>\n",
    "Essential libraries for linear algebra operations and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scientific computing library\n",
    "import numpy as np\n",
    "\n",
    "# visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show plots without need of calling `.show()`\n",
    "%matplotlib inline\n",
    "\n",
    "# prettify plots\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Helper Functions<a class=\"anchor\" id=\"helper-functions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Function<a class=\"anchor\" id=\"visualization\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(x, y=None, y_hat=None):\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    if y is not None:\n",
    "        plt.plot(x, y, '-', label='Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction<a class=\"anchor\" id=\"data-extraction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load files\n",
    "def loader(filename):\n",
    "    c=np.loadtxt('./data/%s.txt' % filename, delimiter=',')\n",
    "    x=c[:,:-1] #extract every column except the last column\n",
    "    y=c[:,-1] #extract the last column\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">!</font> Normalise Function<a class=\"anchor\" id=\"normalise\"></a>\n",
    "$$x_{j}=\\frac{x_{j}-\\mu_{j}}{\\sigma_{j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(X):\n",
    "    #TO-DO#\n",
    "    #Implement normalisation function that normalises the features to be within the same range\n",
    "    #Make use of np.mean() and np.std() with axis=0 to find the mean and standard deviations of each feature.\n",
    "    \n",
    "    return normalised_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">!</font> Model Function<a class=\"anchor\" id=\"model\"></a>\n",
    "$$\\mathbf{\\hat{y}}=\\mathbf{X}*\\mathbf{W}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_linear_model(W,X):\n",
    "    #TO-DO#\n",
    "    #Implement function that outputs y_hat for a m x n size dataset of x and n x 1 size array of weights\n",
    "    #HINT: Make use of np.dot() dot product function\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">!</font> Loss Function- Mean Square Error<a class=\"anchor\" id=\"loss\"></a>\n",
    "\n",
    "$$MSE = \\frac{1}{2m} (\\mathbf{y} - \\mathbf{\\hat{y}})^{\\mathbf{T}}(\\mathbf{y} - \\mathbf{\\hat{y}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE(y, y_hat,size):\n",
    "    #TO-DO#\n",
    "    #Calculate the mean squared error based on the difference between model predictions and observed data.\n",
    "    #HINT: It is EXACTLY the same as for simple linear regression. Literally.\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">!</font> Gradient Descent Algorithm<a class=\"anchor\" id=\"gradient-descent\"></a>\n",
    "$$\\mathbf{W} = \\mathbf{W} - \\alpha \\frac{1}{m}\\mathbf{X}^{\\mathbf{T}}(\\mathbf{\\hat{y}} - \\mathbf{{y}})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(W,X,y,y_hat,size,learning_rate=0.01):\n",
    "    #TO-DO#\n",
    "    #Implement function that adjusts the n x 1 weight matrix, W based on the gradient descent algorithm\n",
    "    #HINT: Make use of np.dot() dot product function and np.transpose() that transposes the matrix\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model<a class=\"anchor\" id=\"train-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise Training Variables<a class=\"anchor\" id=\"initialise\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "weights are:  [0.5488135  0.71518937 0.60276338]\n"
     ]
    }
   ],
   "source": [
    "#Generate X and y matrices\n",
    "X,y=loader(\"ex1data2\")\n",
    "\n",
    "#Normalise features\n",
    "X=normalise(X)\n",
    "\n",
    "n_epochs = 50 #number of iterations\n",
    "np.random.seed(0) #seeding to persist results\n",
    "m=y.size #Get total number of data samples\n",
    "\n",
    "#Add column of ones to X matrix\n",
    "X=np.hstack((np.ones(m)[:, np.newaxis], X))\n",
    "\n",
    "n=X.shape[1] #Get total number of features\n",
    "\n",
    "#Generate N Random Weights\n",
    "W=np.random.rand(n)\n",
    "print(\"weights are: \", W)\n",
    "\n",
    "#initialise arrays to track iteration number and loss\n",
    "it=[] \n",
    "loss_array=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Script<a class=\"anchor\" id=\"training-script\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loss is:  65591252666.364655\n",
      "W is:  [34041.7598896  10577.02326694  5471.38461437]\n",
      "1\n",
      "loss is:  53268030664.0511\n",
      "W is:  [64678.84985809 19789.35469489  9802.84971362]\n",
      "2\n",
      "loss is:  43388905716.139404\n",
      "W is:  [92252.23082973 27837.90513982 13185.30795543]\n",
      "3\n",
      "loss is:  35457290700.17751\n",
      "W is:  [117068.2737042  34892.1939717  15778.8279439]\n",
      "4\n",
      "loss is:  29080444998.437717\n",
      "W is:  [139402.71229123  41095.82530667  17717.97889394]\n",
      "5\n",
      "loss is:  23946992575.4911\n",
      "W is:  [159503.70701955  46570.50741451  19115.83173944]\n",
      "6\n",
      "loss is:  19809504697.12851\n",
      "W is:  [177594.60227504  51419.44613656  20067.33505693]\n",
      "7\n",
      "loss is:  16470956727.16628\n",
      "W is:  [193876.40800498  55730.20992116  20652.16337666]\n",
      "8\n",
      "loss is:  13774160325.483423\n",
      "W is:  [208530.03316193  59577.14885937  20937.12022991]\n",
      "9\n",
      "loss is:  11593489904.529146\n",
      "W is:  [221718.29580319  63023.43725451  20978.16543359]\n",
      "10\n",
      "loss is:  9828384944.804935\n",
      "W is:  [233587.73218031  66122.79841334  20822.1252691 ]\n",
      "11\n",
      "loss is:  8398232004.416896\n",
      "W is:  [244270.22491973  68920.96119376  20508.13506072]\n",
      "12\n",
      "loss is:  7238322438.349826\n",
      "W is:  [253884.4683852   71456.89011805  20068.85593424]\n",
      "13\n",
      "loss is:  6296651644.526091\n",
      "W is:  [262537.28750413  73763.82434035  19531.5010174 ]\n",
      "14\n",
      "loss is:  5531378727.493936\n",
      "W is:  [270324.82471116  75870.15525347  18918.70084153]\n",
      "15\n",
      "loss is:  4908805991.891511\n",
      "W is:  [277333.60819749  77800.16787542  18249.23305997]\n",
      "16\n",
      "loss is:  4401768741.754125\n",
      "W is:  [283641.51333519  79574.66723537  17538.63767934]\n",
      "17\n",
      "loss is:  3988349771.0244665\n",
      "W is:  [289318.62795912  81211.50767019  16799.7356922 ]\n",
      "18\n",
      "loss is:  3650851405.0012765\n",
      "W is:  [294428.03112065  82726.04014954  16043.06620756]\n",
      "19\n",
      "loss is:  3374972281.2318354\n",
      "W is:  [299026.49396604  84131.49039054  15277.25481954]\n",
      "20\n",
      "loss is:  3149147212.0203447\n",
      "W is:  [303165.11052688  85439.27853364  14509.3239661 ]\n",
      "21\n",
      "loss is:  2964017182.7967615\n",
      "W is:  [306889.86543164  86659.28947175  13744.95435097]\n",
      "22\n",
      "loss is:  2812003367.744386\n",
      "W is:  [310242.14484592  87800.10150769  12988.70508592]\n",
      "23\n",
      "loss is:  2686964410.535997\n",
      "W is:  [313259.19631878  88869.17981871  12244.19901473]\n",
      "24\n",
      "loss is:  2583920448.382725\n",
      "W is:  [315974.54264434  89873.04019695  11514.27867136]\n",
      "25\n",
      "loss is:  2498830701.243202\n",
      "W is:  [318418.35433736  90817.38768278  10801.13747337]\n",
      "26\n",
      "loss is:  2428414097.32992\n",
      "W is:  [320617.78486107  91707.23398832  10106.43003302]\n",
      "27\n",
      "loss is:  2370004509.9220567\n",
      "W is:  [322597.27233241  92546.99700163   9431.3648619 ]\n",
      "28\n",
      "loss is:  2321433854.693746\n",
      "W is:  [324378.81105661  93340.58514927   8776.78223321]\n",
      "29\n",
      "loss is:  2280937631.560114\n",
      "W is:  [325982.1959084   94091.4689626    8143.21953375]\n",
      "30\n",
      "loss is:  2247078561.0626245\n",
      "W is:  [327425.24227501  94802.74182796   7530.96607326]\n",
      "31\n",
      "loss is:  2218684818.0034547\n",
      "W is:  [328723.98400495  95477.17159265   6940.10901103]\n",
      "32\n",
      "loss is:  2194800048.0445647\n",
      "W is:  [329892.8515619   96117.24443859   6370.57180003]\n",
      "33\n",
      "loss is:  2174642900.769144\n",
      "W is:  [330944.83236316  96725.20221592   5822.14632992]\n",
      "34\n",
      "loss is:  2157574252.5329432\n",
      "W is:  [331891.61508429  97303.07424325   5294.51976513]\n",
      "35\n",
      "loss is:  2143070645.9457784\n",
      "W is:  [332743.71953331  97852.70442509   4787.29691846]\n",
      "36\n",
      "loss is:  2130702757.219286\n",
      "W is:  [333510.61353742  98375.77440469   4300.01886882]\n",
      "37\n",
      "loss is:  2120117931.5970924\n",
      "W is:  [334200.81814113  98873.82335904   3832.17842073]\n",
      "38\n",
      "loss is:  2111026011.581581\n",
      "W is:  [334822.00228446  99348.26494874   3383.23290939]\n",
      "39\n",
      "loss is:  2103187831.4265072\n",
      "W is:  [335381.06801346  99800.40185576   2952.61477612]\n",
      "40\n",
      "loss is:  2096405871.3723485\n",
      "W is:  [335884.22716956 100231.4382753    2539.7402722 ]\n",
      "41\n",
      "loss is:  2090516661.9684\n",
      "W is:  [336337.07041005 100642.49067105   2144.01659279]\n",
      "42\n",
      "loss is:  2085384607.0517025\n",
      "W is:  [336744.6293265  101034.59705548   1764.84769523]\n",
      "43\n",
      "loss is:  2080896957.1525514\n",
      "W is:  [337111.43235129 101408.72501621   1401.63901584]\n",
      "44\n",
      "loss is:  2076959716.1759152\n",
      "W is:  [337441.55507361 101765.77867575   1053.80126557]\n",
      "45\n",
      "loss is:  2073494305.5067606\n",
      "W is:  [337738.6655237  102106.6047426     720.75345637]\n",
      "46\n",
      "loss is:  2070434843.0887575\n",
      "W is:  [338006.06492877 102431.99778782    401.9252861 ]\n",
      "47\n",
      "loss is:  2067725922.0486066\n",
      "W is:  [3.38246724e+05 1.02742705e+05 9.67589895e+01]\n",
      "48\n",
      "loss is:  2065320795.3064368\n",
      "W is:  [ 3.38463318e+05  1.03039430e+05 -1.95289255e+02]\n",
      "49\n",
      "loss is:  2063179890.3145266\n",
      "W is:  [338658.25207776 103322.83549091   -474.74828299]\n"
     ]
    }
   ],
   "source": [
    "#Initialise Model\n",
    "y_hat=multi_linear_model(W,X)\n",
    "\n",
    "#Iterate Training\n",
    "for _ in range(n_epochs):\n",
    "    it.append(_)\n",
    "    print(_)\n",
    "    \n",
    "    #Calculate loss\n",
    "    loss=calculate_MSE(y,y_hat,m)\n",
    "    loss_array.append(loss)\n",
    "    print(\"loss is: \",loss)\n",
    "\n",
    "    #Perform gradient descent\n",
    "    W = gradient_descent(W,X,y,y_hat,m,0.1)\n",
    "\n",
    "    #Generate new model\n",
    "    y_hat = multi_linear_model(W, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot graph<a class=\"anchor\" id=\"plot-graph\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize(it, loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place to try snippets of code and log the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
